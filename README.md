åŒ»ç–—é—®ç­”å¤§æ¨¡å‹å¾®è°ƒé¡¹ç›®
æœ¬é¡¹ç›®é€šè¿‡ PEFT (Parameter-Efficient Fine-Tuning) æŠ€æœ¯ï¼Œåœ¨åŒ»ç–—å¯¹è¯æ•°æ®é›†ä¸Šå¾®è°ƒäº† Qwen å¤§è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨ä½¿å…¶èƒ½å¤Ÿå¯¹ä¸­æ–‡åŒ»ç–—é—®é¢˜æä¾›ä¸“ä¸šã€å‡†ç¡®çš„å›ç­”ã€‚

ğŸš€ ä¸»è¦ç‰¹æ€§
Qwen æ¨¡å‹: åŸºäº Qwen æ¨¡å‹ï¼Œå…·å¤‡å¼ºå¤§çš„è¯­è¨€ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›ã€‚

PEFT é«˜æ•ˆå¾®è°ƒ: é‡‡ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒï¼Œå¤§å¹…é™ä½è®­ç»ƒèµ„æºæ¶ˆè€—ã€‚

åŒ»ç–—é¢†åŸŸä¸“ä¸šåŒ–: é’ˆå¯¹ä¸­æ–‡åŒ»ç–—é—®ç­”åœºæ™¯è¿›è¡Œä¼˜åŒ–ã€‚

æ”¯æŒé‡åŒ–åŠ è½½: æ”¯æŒ 4-bit / 8-bit é‡åŒ–ï¼Œä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨ã€‚

ğŸ› ï¸ å®‰è£…
å…‹éš†ä»“åº“:

Bash

git clone [ä½ çš„ä»“åº“URL]
cd [ä½ çš„é¡¹ç›®æ–‡ä»¶å¤¹]
å®‰è£…ä¾èµ–:
è¯·ç¡®ä¿å·²å®‰è£… Python 3.8+ã€‚æ ¸å¿ƒä¾èµ–åŒ…æ‹¬ datasets, transformers, peft, accelerate, torch ç­‰ã€‚å»ºè®®åˆ›å»º requirements.txt æ–‡ä»¶å¹¶å®‰è£…ï¼š

requirements.txt ç¤ºä¾‹:

datasets>=3.6.0
transformers
peft
accelerate
torch
# å…¶ä»–ä½ åœ¨é¡¹ç›®ä¸­ä½¿ç”¨çš„åº“ï¼Œä¾‹å¦‚ fsspec, pandas, numpy ç­‰
Bash

pip install -r requirements.txt
ğŸš€ ä½¿ç”¨æ–¹æ³•
æ•°æ®æ ¼å¼
é¡¹ç›®ä½¿ç”¨äº†ç±»ä¼¼ä»¥ä¸‹æ ¼å¼çš„åŒ»ç–—å¯¹è¯æ•°æ®ï¼š

JSON

{'conversations': [{'content': 'æ„Ÿå†’æœ‰ä»€ä¹ˆç—‡çŠ¶ï¼Ÿ', 'role': 'user'}, {'content': 'æ„Ÿå†’çš„ä¸»è¦ç—‡çŠ¶åŒ…æ‹¬æµé¼»æ¶•ã€å’³å—½ã€å–‰å’™ç—›å’Œå‘çƒ­ã€‚', 'role': 'assistant'}]}
è¯·ç¡®ä¿ä½ çš„æ•°æ®é›†éµå¾ªæ­¤æ ¼å¼ã€‚

æ¨¡å‹è®­ç»ƒ
é¡¹ç›®é€šè¿‡å¾®è°ƒ Qwen æ¨¡å‹æ¥å®ç°åŒ»ç–—é—®ç­”èƒ½åŠ›ã€‚è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬åŠ è½½æ•°æ®é›†ï¼Œä½¿ç”¨ PEFT è¿›è¡Œè®­ç»ƒï¼Œå¹¶å°†å¾®è°ƒåçš„æ¨¡å‹æƒé‡å’Œ tokenizer ä¿å­˜åˆ° ./qwen-medical-peft/ ç›®å½•ã€‚

è®­ç»ƒæ—¥å¿—æ¦‚è§ˆ:

è¯»å–æ•°æ®æ¡æ•°ï¼š134

æ•°æ®é¢„å¤„ç† (Map: 100%)

è®­ç»ƒ 3 ä¸ª Epochï¼Œæ€»è®¡ 90 æ­¥

è®­ç»ƒæŸå¤±æŒç»­é™ä½

æ¨¡å‹æ¨ç†
æ¨¡å‹è®­ç»ƒå®Œæˆåï¼Œä½ å¯ä»¥åŠ è½½ qwen-medical-peft ç›®å½•ä¸‹çš„æ¨¡å‹è¿›è¡Œæ¨ç†ï¼š

Python

from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch

# åŠ è½½åˆ†è¯å™¨å’ŒåŸºåº§æ¨¡å‹ï¼ˆè¯·æ›¿æ¢ä¸ºå®é™…ä½¿ç”¨çš„QwenåŸºåº§æ¨¡å‹åç§°ï¼‰
base_model_name = "Qwen/Qwen-7B-Chat"
tokenizer = AutoTokenizer.from_pretrained("./qwen-medical-peft/", trust_remote_code=True)
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    torch_dtype=torch.bfloat16, # æ ¹æ®ä½ çš„è®­ç»ƒé…ç½®è°ƒæ•´
    device_map="auto",
    trust_remote_code=True
)

# åŠ è½½ PEFT å¾®è°ƒåçš„é€‚é…å™¨æƒé‡
model = PeftModel.from_pretrained(base_model, "./qwen-medical-peft/")
model = model.eval() # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼

# æé—®ç¤ºä¾‹
user_query = "èƒƒç–¼åƒä»€ä¹ˆè¯ï¼Ÿ"
messages = [{"role": "user", "content": user_query}]
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512)
response = tokenizer.batch_decode(generated_ids[0][len(model_inputs.input_ids[0]):], skip_special_tokens=True)[0]

print(f"### ç”¨æˆ·:\n{user_query}\n\n### åŒ»ç”Ÿ:\n{response}")
æ¨ç†ç¤ºä¾‹è¾“å‡º:

### ç”¨æˆ·:
èƒƒç–¼åƒä»€ä¹ˆè¯ï¼Ÿ

### åŒ»ç”Ÿ:
æ¸…æ·¡é¥®é£Ÿã€æ­¢ç—›è¯ç­‰ã€‚
